{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"##################### Importin necessary laibraries ############\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport sys\nimport cv2\nimport math\nimport numpy as np\nimport torch.utils.data\nimport torchvision.transforms as transforms\nimport PIL\nimport random\nfrom scipy import ndimage\nimport glob\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\nfrom torch.utils.data import DataLoader,Dataset\nfrom torchvision import transforms\nfrom torchvision.utils import save_image\nfrom time import time\nfrom skimage.data import astronaut,rocket\nfrom skimage.metrics import structural_similarity\nfrom PIL import Image\nfrom statistics import mean\n# %pip install -U git+https://github.com/szagoruyko/pytorchviz.git@master\n# from torchviz import make_dot\nfrom torch.autograd import Variable\ndevice = ('cuda' if torch.cuda.is_available() else 'cpu')\n# !pip install torchmetrics","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-08-29T07:26:54.798849Z","iopub.execute_input":"2022-08-29T07:26:54.799342Z","iopub.status.idle":"2022-08-29T07:27:06.126378Z","shell.execute_reply.started":"2022-08-29T07:26:54.799242Z","shell.execute_reply":"2022-08-29T07:27:06.125102Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"################# Creating datasets ##############\n\n################# Training Dataset ############\nclass train_Dataset(torch.utils.data.Dataset):\n    def __init__(self):\n        super(train_Dataset, self).__init__()\n        self.paths_train=glob.glob('../input/massachusetts-roads-dataset/tiff/train/*.tiff')\n        self.paths_train.sort()\n        self.paths_train_label=glob.glob('../input/massachusetts-roads-dataset/tiff/train_labels/*.tif')\n        self.paths_train_label.sort()\n        \n    def __getitem__(self,idx):\n        image =Image.open(self.paths_train[idx])\n        label =Image.open(self.paths_train_label[idx])\n#         kernel = np.array([[0,0,-1,0,0],\n#                           [0,-1,-2,-1,0],\n#                           [-1,-2,16,-2,-1],\n#                           [0,-1,-2,-1,0],\n#                           [0,0,-1,0,0]])\n        kernel = np.array([[0,-1,0],\n                          [-1,5,-1],\n                          [0,-1,0]])\n        np_img=np.array(image)\n        convolved_img = cv2.filter2D(np_img , -1 , kernel)\n        image_tensor = transforms.ToTensor()(image)\n        label_tensor = transforms.ToTensor()(label)\n        image_resized=image_tensor[:,328:840,328:840]\n        label_resized=label_tensor[:,328:840,328:840]\n        return image_resized,label_resized\n          \n    def __len__(self):\n        return len(self.paths_train_label)\n\n################# Testing Dataset ############\nclass test_Dataset(torch.utils.data.Dataset):\n    def __init__(self):\n        super(test_Dataset, self).__init__()\n        self.paths_test=glob.glob('../input/massachusetts-roads-dataset/tiff/val/*.tiff')\n        self.paths_test.sort()\n        self.paths_test_label=glob.glob('../input/massachusetts-roads-dataset/tiff/val_labels/*.tif')\n        self.paths_test.sort()\n\n    def __getitem__(self,idx):\n        image =Image.open(self.paths_test[idx])\n        label =Image.open(self.paths_test_label[idx])\n#         kernel = np.array([[0,0,-1,0,0],\n#                           [0,-1,-2,-1,0],\n#                           [-1,-2,16,-2,-1],\n#                           [0,-1,-2,-1,0],\n#                           [0,0,-1,0,0]])\n        kernel = np.array([[0,-1,0],\n                          [-1,5,-1],\n                          [0,-1,0]])\n        \n        np_img=np.array(image)\n        convolved_img = cv2.filter2D(np_img , -1 , kernel)\n        image_tensor = transforms.ToTensor()(convolved_img)\n        label_tensor = transforms.ToTensor()(label)\n        image_resized=image_tensor[:,328:840,328:840]\n        label_resized=label_tensor[:,328:840,328:840]\n        return image_resized,label_resized\n    \n    def __len__(self):\n        return len(self.paths_test_label)","metadata":{"execution":{"iopub.status.busy":"2022-08-29T07:27:06.128242Z","iopub.execute_input":"2022-08-29T07:27:06.129351Z","iopub.status.idle":"2022-08-29T07:27:06.146980Z","shell.execute_reply.started":"2022-08-29T07:27:06.129310Z","shell.execute_reply":"2022-08-29T07:27:06.145764Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"################# Defining Dataloaders ############\n'''Dataloader provides desired data from dataset with specific batch size '''\n\ntrain_data=train_Dataset()\ntest_data=test_Dataset()\ntrain_loader = DataLoader(train_data, batch_size = 1,num_workers = 0)\ntest_loader = DataLoader(test_data, batch_size = 1, num_workers = 0)","metadata":{"execution":{"iopub.status.busy":"2022-08-29T07:27:06.150157Z","iopub.execute_input":"2022-08-29T07:27:06.150589Z","iopub.status.idle":"2022-08-29T07:27:07.176856Z","shell.execute_reply.started":"2022-08-29T07:27:06.150556Z","shell.execute_reply":"2022-08-29T07:27:07.175575Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"########## Numbers of images in train & test dataset ###########\nprint(len(train_data))\nprint(len(test_data))","metadata":{"execution":{"iopub.status.busy":"2022-07-08T07:12:46.014093Z","iopub.execute_input":"2022-07-08T07:12:46.014427Z","iopub.status.idle":"2022-07-08T07:12:46.020577Z","shell.execute_reply.started":"2022-07-08T07:12:46.014399Z","shell.execute_reply":"2022-07-08T07:12:46.019494Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"################## Observing arial image and ground truth ###############\nfor batch_i, (arial_image,ground_truth) in enumerate(train_loader):\n    if batch_i == 30 :\n        fig, ax = plt.subplots(1,2, figsize =(25,25))\n        ax[0].imshow(arial_image[0].permute(1,2,0))\n        ax[1].imshow(ground_truth[0].permute(1,2,0),'gray')\n        print(batch_i)\n        break","metadata":{"execution":{"iopub.status.busy":"2022-08-29T07:27:13.641160Z","iopub.execute_input":"2022-08-29T07:27:13.642294Z","iopub.status.idle":"2022-08-29T07:27:21.611080Z","shell.execute_reply.started":"2022-08-29T07:27:13.642235Z","shell.execute_reply":"2022-08-29T07:27:21.609850Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"############ Defining classes which are used in Network #############\n\n############ Transition down layer is used in downsampling path generator and discriminator #############\nclass Transition_Down(nn.Module): \n  def __init__(self, input_channels):\n    super(Transition_Down, self).__init__()\n    self.TD = nn.Sequential(nn.Dropout2d(0.5),\n                            nn.Conv2d(input_channels,2*input_channels,kernel_size=4,padding =1,stride=2),\n                            nn.BatchNorm2d(2*input_channels),\n                            nn.LeakyReLU(negative_slope=0.01))\n    \n  def forward(self,x):\n    out = self.TD(x)\n    return out  \n    \n####################################################################################################\n\n############ Transition up layer is used in upsampling path of generator  #############\nclass Transition_Up(nn.Module): \n  def __init__(self, input_channels,output_channels):\n    super(Transition_Up, self).__init__()\n    self.TU = nn.Sequential(nn.Dropout2d(0.5),\n                            nn.ConvTranspose2d(input_channels,output_channels,kernel_size=4,padding=1,stride=2),\n                            nn.BatchNorm2d(output_channels),\n                            nn.ReLU())\n  def forward(self,x):\n    out = self.TU(x)\n    return out  \n     \n    ","metadata":{"execution":{"iopub.status.busy":"2022-08-29T07:27:34.017962Z","iopub.execute_input":"2022-08-29T07:27:34.018358Z","iopub.status.idle":"2022-08-29T07:27:34.029509Z","shell.execute_reply.started":"2022-08-29T07:27:34.018326Z","shell.execute_reply":"2022-08-29T07:27:34.028031Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"############ Defining Generator & Discriminator by using the classes which are defined previously #############\n############ Generator is used to produce segmentation map ############\nclass Generator(nn.Module):\n  def __init__(self):\n    super(Generator,self).__init__()\n    self.conv  = nn.Conv2d( 3, 32, kernel_size = 4 , padding = 1 ,stride=2)\n    self.LReLU = nn.LeakyReLU(negative_slope=0.1)\n    self.TD1   = Transition_Down(32)\n    self.TD2   = Transition_Down(64)\n    self.TD3   = Transition_Down(128)\n    self.TD4   = Transition_Down(256)\n    self.TU1   = Transition_Up (512,256)\n    self.TU2   = Transition_Up (512,128)\n    self.TU3   = Transition_Up (256,64)\n    self.TU4   = Transition_Up (128,32)\n    self.deconv= nn.ConvTranspose2d(64,1,kernel_size=4,padding=1,stride=2)\n    self.relu = nn.ReLU()\n    self.Tan  = nn.Tanh()\n  def forward(self, input):\n    y   = self.LReLU(self.conv(input))\n    TD1 = self.TD1(y)\n    TD2 = self.TD2(TD1)\n    TD3 = self.TD3(TD2)\n    TD4 = self.TD4(TD3)\n    TU1 = self.TU1(TD4)\n    C1  = torch.cat((TD3, TU1), 1) # Concatenation\n    TU2 = self.TU2(C1)\n    C2  = torch.cat((TD2, TU2), 1)\n    TU3 = self.TU3(C2)\n    C3  = torch.cat((TD1, TU3), 1)\n    TU4 = self.TU4(C3)\n    C4  = torch.cat((y, TU4), 1)\n    output = self.Tan(self.deconv(C4))\n    return output\n\n############### Discriminator is used to distinguish between ground truth & fake segmentations ######## \nclass Discriminator(nn.Module):\n  def __init__(self):\n    super(Discriminator,self).__init__()\n    self.conv1 = nn.Conv2d( 4, 32, kernel_size = 4 , padding = 1 ,stride=2)\n    self.LReLU = nn.LeakyReLU(negative_slope=0.1)\n    self.TD1   = Transition_Down(32)\n    self.TD2   = Transition_Down(64)\n    self.TD3   = Transition_Down(128)\n    self.TD4   = Transition_Down(256)\n    self.conv2 = nn.Conv2d( 512, 1, kernel_size = 4 , padding = 1 ,stride=2)\n    self.Sigm  = nn.Sigmoid()\n  def forward(self, input,ground_truth):\n    y_input = torch.cat((input,ground_truth),1)\n    y   = self.LReLU(self.conv1(y_input))\n    TD1 = self.TD1(y)\n    TD2 = self.TD2(TD1)\n    TD3 = self.TD3(TD2)\n    TD4 = self.TD4(TD3)\n    output = self.Sigm(self.conv2(TD4))\n    return output","metadata":{"execution":{"iopub.status.busy":"2022-08-29T07:27:36.597672Z","iopub.execute_input":"2022-08-29T07:27:36.598122Z","iopub.status.idle":"2022-08-29T07:27:36.620645Z","shell.execute_reply.started":"2022-08-29T07:27:36.598085Z","shell.execute_reply":"2022-08-29T07:27:36.617801Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"############### Testing the functionality of generator & discriminator by giving random tensor)\ntest_gen = Generator()\ntest_disc = Discriminator()\ninput_of_generator = torch.randn((1,3,512,512)) # random tensor\noutput_of_generator=test_gen.forward(input_of_generator)\noutput_of_discriminator=test_disc.forward(input_of_generator,output_of_generator)\nprint(output_of_generator.shape)\nprint(output_of_discriminator.shape)","metadata":{"execution":{"iopub.status.busy":"2022-08-29T07:27:39.188768Z","iopub.execute_input":"2022-08-29T07:27:39.189444Z","iopub.status.idle":"2022-08-29T07:27:39.853622Z","shell.execute_reply.started":"2022-08-29T07:27:39.189395Z","shell.execute_reply":"2022-08-29T07:27:39.852761Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"############ Plotting architecture of generator & discriminator ###########\niput_of_generator = torch.randn((1,3,512,512))\niput_of_discriminator = torch.randn((1,1,512,512))\nmymodel_generator=Generator()\nmymodel_discriminator=Discriminator()\nmake_dot(mymodel_generator(iput_of_generator), params=dict(mymodel.named_parameters())).render(\"Generator_model.jpeg\")","metadata":{"execution":{"iopub.status.busy":"2022-07-07T20:52:36.115402Z","iopub.execute_input":"2022-07-07T20:52:36.115895Z","iopub.status.idle":"2022-07-07T20:52:36.263565Z","shell.execute_reply.started":"2022-07-07T20:52:36.115841Z","shell.execute_reply":"2022-07-07T20:52:36.261237Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"########## Defining Loss Function classes for each network ###########\nclass MaxminLoss(nn.Module):\n  def __init__(self):\n      super(MaxminLoss,self).__init__()\n        \n  def forward(self,real_output,fake_output):    \n      loss = torch.log(real_output) + torch.log(1-fake_output)\n      return loss\n","metadata":{"execution":{"iopub.status.busy":"2022-08-29T07:27:47.708361Z","iopub.execute_input":"2022-08-29T07:27:47.708904Z","iopub.status.idle":"2022-08-29T07:27:47.715435Z","shell.execute_reply.started":"2022-08-29T07:27:47.708868Z","shell.execute_reply":"2022-08-29T07:27:47.714135Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"################# Initializing networks and parameters part ##############\nD = Discriminator().to(device)\nG = Generator().to(device)\nBCE = nn.BCELoss()\nBCE_with_Sigmoid = nn.BCEWithLogitsLoss()\nMSE=nn.MSELoss()\nd_optim = torch.optim.Adam(D.parameters() , lr = 0.01, betas = (0.9, 0.999))\ng_optim = torch.optim.Adam(G.parameters() , lr = 0.01, betas = (0.9, 0.999))","metadata":{"execution":{"iopub.status.busy":"2022-08-29T07:27:50.759560Z","iopub.execute_input":"2022-08-29T07:27:50.759976Z","iopub.status.idle":"2022-08-29T07:27:50.851476Z","shell.execute_reply.started":"2022-08-29T07:27:50.759940Z","shell.execute_reply":"2022-08-29T07:27:50.850294Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"'''\n DO NOT RUN THIS BLOCK !!! \n this block is used to train the network but because I have trained the model and\n you can see the information of training the trained model is available \n there is no need to train model and another reason that I don't recommend to run\n this block is that it takes a long time.\n If\n'''\n###################### Training Loop #####################\nbatch_size=1\nlambdaa = 0.9\nd_losses = []\ng_losses = []\nN_EPOCHS = 20\n\nfor epoch in range(N_EPOCHS):\n    for batch_i, (arial_image,ground_truth) in enumerate(train_loader):\n        arial_image=Variable(arial_image.to(device))\n        ground_truth=Variable(ground_truth.to(device))\n        #######################\n        ##Training Discriminator##\n        #######################\n        #real segmentations\n        real_outputs = D.forward(ground_truth,arial_image) # >>>> Expected all ones\n        real_outputs_squeezed=real_outputs.squeeze()\n        ones=Variable(torch.ones(real_outputs_squeezed.size()).to(device)) ### producing 8*8 kernel of ones for patchGAN\n        d_loss_real  = BCE(real_outputs_squeezed, ones)\n        #fake segmentations\n        fake_segmentations = G.forward(arial_image)\n        fake_outputs = D.forward(fake_segmentations,arial_image) # >>> Expected all zeros\n        fake_outputs_squeezed=fake_outputs.squeeze()\n        zeros=Variable(torch.zeros(fake_outputs_squeezed.size()).to(device))### producing 8*8 kernel of ones for patchGAN\n        d_loss_fake = BCE(fake_outputs_squeezed, zeros)\n        # GD Step\n        d_loss = 0.5 * (d_loss_real + d_loss_fake)\n        d_optim.zero_grad()\n        g_optim.zero_grad()\n        d_loss.backward()\n        d_optim.step()\n        \n        #######################\n        ####Training Generator####\n        #######################\n        #fake images\n        fake_segmentations = G.forward(arial_image)\n        fake_outputs = D.forward(fake_segmentations,arial_image)\n        fake_outputs_squeezed=fake_outputs.squeeze()\n        #the actual labels are zeros\n        #since we want to trick the discriminator, we say it's ones.\n        g_loss = BCE(fake_outputs_squeezed, ones) ##### VERY IMPORTANT\n        d_optim.zero_grad()\n        g_optim.zero_grad()\n        g_loss.backward()\n        g_optim.step()\n        d_losses.append(d_loss.item())\n        g_losses.append(g_loss.item())\n        if batch_i % 10 == 0 :\n            train_string1 = f\"information of each batch => batch : {batch_i}// g_loss : {g_loss.item(): 0.4f} // d_loss : {d_loss.item(): 0.4f} \"\n            print(train_string1) \n    g_loss_mean=mean(g_losses)\n    d_loss_mean=mean(d_losses)\n    train_string2 = f\"information of each epoch => epoch : {epoch}// g_loss_mean : {g_loss_mean: 0.4f} // d_loss : {d_loss_mean: 0.4f} \"\n    print(train_string2 ) \n    \n","metadata":{"execution":{"iopub.status.busy":"2022-07-08T08:11:43.763783Z","iopub.execute_input":"2022-07-08T08:11:43.764356Z","iopub.status.idle":"2022-07-08T09:07:07.920954Z","shell.execute_reply.started":"2022-07-08T08:11:43.764307Z","shell.execute_reply":"2022-07-08T09:07:07.919882Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"''' Here we have plotted Discriminator and Generator loss together for 150 \n    first samples to show that they are changing and competing with each other '''\nplt.plot(g_losses, label = 'g_losses')\nplt.plot(d_losses, label = 'd_losses')\nplt.legend()","metadata":{"execution":{"iopub.status.busy":"2022-07-08T08:03:05.109593Z","iopub.execute_input":"2022-07-08T08:03:05.110003Z","iopub.status.idle":"2022-07-08T08:03:05.323419Z","shell.execute_reply.started":"2022-07-08T08:03:05.109972Z","shell.execute_reply":"2022-07-08T08:03:05.322445Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"################# Saving the weight of trained generator ######################\ntorch.save(G.state_dict(), 'Road_GAN_Generator_1.pt') \n# torch.save(D.state_dict(), 'Road_GAN_Discriminator1.pt')","metadata":{"execution":{"iopub.status.busy":"2022-07-07T20:53:05.146379Z","iopub.execute_input":"2022-07-07T20:53:05.146959Z","iopub.status.idle":"2022-07-07T20:53:05.27794Z","shell.execute_reply.started":"2022-07-07T20:53:05.146922Z","shell.execute_reply":"2022-07-07T20:53:05.276916Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\n here you can use my saved model that I have sent. just upload it to your colab or kaggle and then write its directory\n here -> (torch.load(directory))\n'''\nmysaved_model=Generator()\nmysaved_model.load_state_dict(torch.load('../input/generator-1/Road_GAN_Generator_1.pt',map_location=torch.device('cpu')))","metadata":{"execution":{"iopub.status.busy":"2022-08-29T07:27:53.630638Z","iopub.execute_input":"2022-08-29T07:27:53.631063Z","iopub.status.idle":"2022-08-29T07:27:54.399930Z","shell.execute_reply.started":"2022-08-29T07:27:53.631027Z","shell.execute_reply":"2022-08-29T07:27:54.398824Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"########################### Quantitative Evaluation Functions ####################### \ndef Pixels_Counter(segmentation,ground_truth):\n    segmentation= (segmentation + 1)/2\n    TP=0\n    TN=0\n    FP=0 \n    FN=0\n    for i in range(1):\n        for j in range(512):\n            for k in range(512):\n                if segmentation[i,j,k] >= 0.9 :\n                    segmentation[i,j,k]=1\n                    if segmentation[i,j,k] == ground_truth[i,j,k]:\n                        TP = TP + 1 \n                    else :\n                        FP = FP + 1 \n                else:\n                    segmentation[i,j,k]=0\n                    if segmentation[i,j,k] == ground_truth[i,j,k]:\n                        TN = TN + 1 \n                    else :\n                        FN = FN + 1 \n\n    return TP,TN,FP,FN\n##################################################################\n############### Matthews Correlation Coeiffivient ################\ndef MCC (TP,TN,FP,FN):\n    MCC= ((TP*TN)-(FP*FN))/math.sqrt((TP+FN)*(TP+FP)*(TN+FN)*(TN+FP))\n    return MCC\n##################################################################\n########################### Recall ###############################\ndef RECALL (TP,FN):\n    recall= TP/(TP+FN)\n    return recall         \n##################################################################\n########################## Precision #############################\ndef Precision (TP,FP):\n    precision= TP/(TP+FP)\n    return precision        \n##################################################################\n########################## F1 Score ##############################\ndef F1 (Precision,Recall):\n    f1= (2*Precision*Recall)/(Precision+Recall)\n    return f1        \n##################################################################\n############### Mean Intersection Over Union ################\ndef MIOU (TP,FP,FN):\n    miou= TP/(TP+FP+FN)\n    return miou        \n          ","metadata":{"execution":{"iopub.status.busy":"2022-08-29T07:27:58.390763Z","iopub.execute_input":"2022-08-29T07:27:58.391525Z","iopub.status.idle":"2022-08-29T07:27:58.405081Z","shell.execute_reply.started":"2022-08-29T07:27:58.391486Z","shell.execute_reply":"2022-08-29T07:27:58.404042Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"##################### Testing the generator model on testing dataset #####################\n'''  In this block we are evaluating method visually and theorically by using metrcis which have been menthioned in the paper'''\n\npath11=glob.glob(\"../input/massachusetts-roads-dataset/tiff/test/10378780_15.tiff\")\npath21=glob.glob(\"../input/massachusetts-roads-dataset/tiff/test_labels/10378780_15.tif\")\npath12=glob.glob(\"../input/massachusetts-roads-dataset/tiff/test/12328750_15.tiff\")\npath22=glob.glob(\"../input/massachusetts-roads-dataset/tiff/test_labels/12328750_15.tif\")\npath13=glob.glob(\"../input/massachusetts-roads-dataset/tiff/test/20728960_15.tiff\")\npath23=glob.glob(\"../input/massachusetts-roads-dataset/tiff/test_labels/20728960_15.tif\")\n\narial_image1 =Image.open(path11[0])\nground_truth1=Image.open(path21[0])\narial_image1 = transforms.ToTensor()(arial_image1)\nground_truth1 = transforms.ToTensor()(ground_truth1)\narial_image1=arial_image1[:,128:640,128:640]\narial_image1=arial_image1.reshape(1,3,512,512)\nground_truth1=ground_truth1[:,128:640,128:640]\nSegmentation1=mysaved_model.forward(arial_image1).detach()\n\nfig, ax = plt.subplots(1,3, figsize =(15,15))\nax[0].imshow(arial_image1[0].permute(1,2,0),)\nax[1].imshow(Segmentation1[0].permute(1,2,0),'gray')\nax[2].imshow(ground_truth1.permute(1,2,0),'gray')\nTP1,TN1,FP1,FN1=Pixels_Counter(Segmentation1[0],ground_truth1)\nmcc=MCC(TP1,TN1,FP1,FN1)\nrecall=RECALL(TP1,FN1)\nprecision=Precision(TP1,FP1)\nf1=F1(precision,recall)\nmiou=MIOU(TP1,FP1,FN1)\nprint(f\" True_Positive = {TP1} //  True_Nagative = {TN1} //  False_Positive = {FP1} //  False_Nagative = {FN1}\")\nprint(f\" MCC : {mcc: 0.4f} // Reacll : {recall: 0.4f} // Precision : {precision: 0.4f} // F1 : {f1: 0.4f} // MIOU : {miou: 0.4f}\")      \n\narial_image2 =Image.open(path12[0])\nground_truth2=Image.open(path22[0])\narial_image2 = transforms.ToTensor()(arial_image2)\nground_truth2 = transforms.ToTensor()(ground_truth2)\narial_image2=arial_image2[:,128:640,128:640]\narial_image2=arial_image2.reshape(1,3,512,512)\nground_truth2=ground_truth2[:,128:640,128:640]\nSegmentation2=mysaved_model.forward(arial_image2).detach()\n\nfig, ax = plt.subplots(1,3, figsize =(15,15))\nax[0].imshow(arial_image2[0].permute(1,2,0),)\nax[1].imshow(Segmentation2[0].permute(1,2,0),'gray')\nax[2].imshow(ground_truth2.permute(1,2,0),'gray')\nTP2,TN2,FP2,FN2=Pixels_Counter(Segmentation2[0],ground_truth2)\nmcc=MCC(TP2,TN2,FP2,FN2)\nrecall=RECALL(TP2,FN2)\nprecision=Precision(TP2,FP2)\nf1=F1(precision,recall)\nmiou=MIOU(TP2,FP2,FN2)\nprint(f\" True_Positive = {TP2} //  True_Nagative = {TN2} //  False_Positive = {FP2} //  False_Nagative = {FN2}\")\nprint(f\" MCC : {mcc: 0.4f} // Reacll : {recall: 0.4f} // Precision : {precision: 0.4f} // F1 : {f1: 0.4f} // MIOU : {miou: 0.4f}\")      \n\narial_image3 =Image.open(path13[0])\nground_truth3=Image.open(path23[0])\narial_image3 = transforms.ToTensor()(arial_image3)\nground_truth3 = transforms.ToTensor()(ground_truth3)\narial_image3=arial_image3[:,128:640,128:640]\narial_image3=arial_image3.reshape(1,3,512,512)\nground_truth3=ground_truth3[:,128:640,128:640]\nSegmentation3=mysaved_model.forward(arial_image3).detach()\n\nfig, ax = plt.subplots(1,3, figsize =(15,15))\nax[0].imshow(arial_image3[0].permute(1,2,0),)\nax[1].imshow(Segmentation3[0].permute(1,2,0),'gray')\nax[2].imshow(ground_truth3.permute(1,2,0),'gray')\nTP3,TN3,FP3,FN3=Pixels_Counter(Segmentation3[0],ground_truth3)\nmcc=MCC(TP3,TN3,FP3,FN3)\nrecall=RECALL(TP3,FN3)\nprecision=Precision(TP3,FP3)\nf1=F1(precision,recall)\nmiou=MIOU(TP3,FP3,FN3)\nprint(f\" True_Positive = {TP3} //  True_Nagative = {TN3} //  False_Positive = {FP3} //  False_Nagative = {FN3}\")\nprint(f\" MCC : {mcc: 0.4f} // Reacll : {recall: 0.4f} // Precision : {precision: 0.4f} // F1 : {f1: 0.4f} // MIOU : {miou: 0.4f}\")      \n","metadata":{"execution":{"iopub.status.busy":"2022-08-29T07:28:00.121861Z","iopub.execute_input":"2022-08-29T07:28:00.122270Z","iopub.status.idle":"2022-08-29T07:28:24.309912Z","shell.execute_reply.started":"2022-08-29T07:28:00.122234Z","shell.execute_reply":"2022-08-29T07:28:24.309009Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}